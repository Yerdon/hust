# GPT-3训练费用的两个说法
## 说法一
训练GPT-3，需要`3.14E23 FLOPS`，在GPT-3问世时，最高端的GPU是V100，其理论算力是`28 TFLOPS`，那么需要` 355个 “GPU年”` (GPU-year，即V100运行一年) 才能训练完成。选择一个价格很优惠的公有云（lambda labs），使用公有云“连续包三年”的套餐，V100的使用费用可以低至`$1.5/H`。
那么`355Y×365D/Y×24H/D×$1.5/H=$4.6M`

## 说法二
另外一个说法是需要1200万美元，这个说法来自`Elliot Turner`的[推文](https://twitter.com/eturner303/status/1266264358771757057),`Elliot Turner` 是`AlchemyAPI`的founder，把`AIchemyAPI`成功卖给了`IBM`后，成了投资人。`Elliot Turner` 没有给出1200万美元的算法，也许他有朋友在`OpenAI`，anyway，这比460万美元要靠谱的多。


# 如果使用已经训练的GPT-3，是什么价格？
一位名为 `Gwern Branwen` 的研究员在 Reddit 上发布了详细的定价信息，其中共包括四个方案：
```
- Explore 版：免费套餐：用户可免费使用 3 个月或 10 万 token，哪个先用完就以那个为基准。
- Create 版：100 美元/月、200 万 token/月，超过部分则按 1k tokens 消耗 8 美分计算。
- Build 版：400 美元/月、1000 万 token/月 ，超出部分则按 1k tokens 消耗 6 美分计算。
- Scal 版：没有给出具体定价，需联系官方以了解详情。
```
何为 token？token 是将文本分解为较小的单元，通常是单词或字符。为了方便大众理解 GPT-3 基于 token 的定价模型，`Branwen` 解释称，200 万 token 将对应 3000 页文本。  
**那么海量的文书又对应多少token呢？又对应多少美元呢？**

# 为什么这么贵？
## 数据与计算量的指数爆炸
一个适用于所有统计学模型的规则是：
```
- 要想使性能提高k倍，至少需要k^2倍的数据来训练模型。
- 又因为深度学习模型的过参数化，使性能提高k倍将需要至少k^4倍的计算量。
```
## 一个形象的比喻
喂一个模型就像养一个孩子。  
我们希望把这个孩子养成法学博士和法学本科就好了的成本不是一个量级的。


1. 孩子的天资（硬件好不好）。这决定了孩子学东西快不快（显卡）、脑容量大小（能存多少数据、知识）。如果速度太慢，技术迭代了模型没训练好。等于说三孩都开放了，孩子的两孩政策博士论文还没写完。如果对于V100（一张卡好几万，哪怕回头可以卖）来说，训练GPT-3这样的模型需要355卡年，那如果我们要在一年内训练出来，500万铁不够；内存和硬盘的成本倒不是大头。
2. 海量的学习资料（数据量够不够大）。天资再聪慧的孩子，我们也不能指望他的知识可以凭空产生，获取学习资料的过程也存在很大的成本。
3. 好的家长/老师。这就是一支团队了，好老师的价格和一般老师的价格也不是一个概念。